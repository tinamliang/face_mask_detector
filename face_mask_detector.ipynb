{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0) # Video source capturing\n",
    "cap.set(3, 640) # Width of the video window\n",
    "cap.set(4, 480) # Height of the video window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') # Face detector\n",
    "maskClassifier = load_model('maskclassifier.model') # Mask classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "   \n",
    "    (rval, frame) = cap.read() # Reading frame from video source\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) # Converting RGB to Grayscale\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale( # Detecting faces\n",
    "        gray,\n",
    "        scaleFactor = 1.2,\n",
    "        minNeighbors = 5,\n",
    "        )\n",
    "    \n",
    "    for (x, y, h, w) in faces: \n",
    "\n",
    "        faceROI = frame[y : y + h, x : x + w, :] # Cropping face region of interest\n",
    "\n",
    "        faceROI = cv2.resize(faceROI, (160, 160)) # Resizing faceROI to 160x160\n",
    "                                                  # Because, Our VGG16 model accepts 160x160 as input \n",
    "        faceROI = img_to_array(faceROI)\n",
    "        faceROI = faceROI.reshape(1, 160, 160, 3) # Changing dimensions to 1x160x160x3, Because our VGG16 \n",
    "                                                  # take input as 4D matrix(BATCH_SIZE, 160, 160, #Channels)\n",
    "\n",
    "        prediction = maskClassifier(faceROI) # Making predictions\n",
    "        (withoutmask, withmask) = prediction[0].numpy()\n",
    "        \n",
    "        # Drawing bounding boxes using OpenCV\n",
    "        (label, color, prob) = ('Mask', (0, 255, 0), withmask*100.0) if withmask > withoutmask else ('No mask', (0, 0, 255), withoutmask*100.0)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "        cv2.rectangle(frame, (x + 15, y + 2), (x + w - 15, y + 20), (0, 0, 0), -1) #lower\n",
    "        cv2.rectangle(frame, (x + 15, y - 2), (x + w - 15, y - 20), (0, 0, 0), -1) #upper\n",
    "\n",
    "        cv2.putText(frame, str(prob)+' %', (x + 20, y - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        cv2.putText(frame, label, (x + 20, y + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "\n",
    "        \n",
    "    cv2.imshow('Video', frame) # Displaying the video\n",
    "    \n",
    "\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release() # Releasing the capture\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1df983b910996cb3f01687c27c702b690abf3e9959b8282b50a3f692d59a66f9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
